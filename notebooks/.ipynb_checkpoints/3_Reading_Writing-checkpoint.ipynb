{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pandas: Data Loading and Storing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parsing functions in pandas\n",
    "Function | Description\n",
    "---------|------------\n",
    "read_csv | Load delimited data from a file, URL, or file-like object. Use comma as default delimiter\n",
    "read_table | Load delimited data from a file, URL, or file-like object. Use tab ('\\t') as default delimiter\n",
    "read_fwf | Read data in fixed-width column format (that is, no delimiters)\n",
    "read_clipboard | Version of read_table that reads data from the clipboard. Useful for converting tables from web pages\n",
    "\n",
    "*Type inference* is one of the more important features of these functions; that means you don’t have to specify which columns are numeric, integer, boolean, or string.\n",
    "\n",
    "There are *tons* of arguments for these functions, such as `header=`, `names=`, `index_col=`. In the interest of brevity they will not be covered here; please refer to the pandas documentation or the `Help` functionality within your IDE of choice. *(Wes McKinney's book, referenced below, does go into a fair amount of detail; I highly recommend his book!)* The table below provides a brief description of the read_table arguments.\n",
    "\n",
    "I will cover one here: `na_values`.  \n",
    "\n",
    "The na_values option can take either a list or set of strings to consider missing values:\n",
    ">result = pd.read_csv('ch06/ex5.csv', na_values=['NULL'])\n",
    "\n",
    "Different NA sentinels can be specified for each column in a dict:\n",
    ">sentinels = {'message': ['foo', 'NA'], 'something': ['two']}  \n",
    ">pd.read_csv('ch06/ex5.csv', na_values=sentinels)  \n",
    "\n",
    "### read_csv /read_table function arguments\n",
    "Argument | Description\n",
    "---------|------------\n",
    "path | String indicating filesystem location, URL, or file-like object\n",
    "sep or delimiter | Character sequence or regular expression to use to split fields in each row\n",
    "header | Row number to use as column names. Defaults to 0 (first row), but should be None if there is no header row\n",
    "index_col | Column numbers or names to use as the row index in the result. Can be a single name/number or a list of them for a hierarchical index\n",
    "names | List of column names for result, combine with header=None\n",
    "skiprows | Number of rows at beginning of file to ignore or list of row numbers (starting from 0) to skip\n",
    "na_values | Sequence of values to replace with NA\n",
    "comment | Character or characters to split comments off the end of lines\n",
    "parse_dates | Attempt to parse data to datetime; False by default. If True, will attempt to parse all columns. Otherwise can specify a list of column numbers or name to parse. If element of list is tuple or list, will combine multiple columns together and parse to date (for example if date/time split across two columns)\n",
    "keep_date_col | If joining columns to parse date, drop the joined columns. Default True\n",
    "converters | Dict containing column number of name mapping to functions. For example {'foo': f} would apply the function f to all values in the 'foo' column dayfirst When parsing potentially ambiguous dates, treat as international format (e.g. 7/6/2012 -> June 7, 2012). Default False\n",
    "date_parser | Function to use to parse dates\n",
    "nrows | Number of rows to read from beginning of file\n",
    "iterator | Return a TextParser object for reading file piecemeal\n",
    "chunksize | For iteration, size of file chunks\n",
    "skip_footer | Number of lines to ignore at end of file\n",
    "verbose | Print various parser output information, like the number of missing values placed in non-numeric columns\n",
    "encoding | Text encoding for unicode. For example 'utf-8' for UTF-8 encoded text\n",
    "squeeze | If the parsed data only contains one column return a Series\n",
    "thousands | Separator for thousands, e.g. ',' or '.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading Text Files in Pieces\n",
    "When processing very large files or figuring out the right set of arguments to correctly process a large file, you may only want to read in a small piece of a file or iterate through\n",
    "smaller chunks of the file.\n",
    "\n",
    ">pd.read_csv('ch06/ex6.csv', nrows=*n*) where *N* is the number of rows to read\n",
    "\n",
    "To read out a file in pieces, specify a chunksize as a number of rows:\n",
    ">chunker = pd.read_csv('ch06/ex6.csv', chunksize=1000)\n",
    "\n",
    "The `TextParser` object returned by read_csv allows you to iterate over the parts of the file according to the chunksize. For example, we can iterate over ex6.csv, aggregating the value counts in the 'key' column like so:\n",
    "\n",
    ">chunker = pd.read_csv('ch06/ex6.csv', chunksize=1000)  \n",
    "tot = Series([])  \n",
    "for piece in chunker:  \n",
    "____tot = tot.add(piece['key'].value_counts(), fill_value=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing Data Out to Text Format\n",
    "\n",
    "Using DataFrame’s to_csv method, we can write the data out to a comma-separated file:\n",
    ">data.to_csv('ch06/out.csv')\n",
    "\n",
    "The `sep=` argument can be used to define a deliminator other than the default comma.\n",
    "\n",
    "Missing values appear as empty strings in the output. You might want to denote them by some other sentinel value:\n",
    ">data.to_csv(sys.stdout, na_rep='NULL')\n",
    "\n",
    "With no other options specified, both the row and column labels are written. Both of these can be disabled:\n",
    ">data.to_csv(sys.stdout, index=False, header=False)\n",
    "\n",
    "You can also write only a subset of the columns, and in an order of your choosing:\n",
    ">data.to_csv(sys.stdout, index=False, cols=['a', 'b', 'c'])\n",
    "\n",
    "### CSV dialect options\n",
    "Argument | Description\n",
    "---------|------------\n",
    "delimiter | One-character string to separate fields. Defaults to ','.\n",
    "lineterminator | Line terminator for writing, defaults to '\\r\\n'. Reader ignores this and recognizes cross-platform line terminators.\n",
    "quotechar |Quote character for fields with special characters (like a delimiter). Default is '\"'.\n",
    "quoting | Quoting convention. Options include csv.QUOTE_ALL (quote all fields), csv.QUOTE_MINIMAL (only fields with special characters like the delimiter), csv.QUOTE_NONNUMERIC, and csv.QUOTE_NON (no quoting). See Python’s documentation for full details. Defaults to QUOTE_MINIMAL.\n",
    "skipinitialspace | Ignore whitespace after each delimiter. Default False.\n",
    "doublequote | How to handle quoting character inside a field. If True, it is doubled. See online documentation for full detail and behavior.\n",
    "escapechar | String to escape the delimiter if quoting is set to csv.QUOTE_NONE. Disabled by default\n",
    "\n",
    "Lastly, to *write* delimited files manually, you can use csv.writer. It accepts an open, writable file object and the same dialect and format options as csv.reader:\n",
    "\n",
    ">with open('mydata.csv', 'w') as f:  \n",
    "----writer = csv.writer(f, dialect=my_dialect)  \n",
    "----writer.writerow(('one', 'two', 'three'))  \n",
    "----writer.writerow(('1', '2', '3'))  \n",
    "----writer.writerow(('4', '5', '6'))  \n",
    "----writer.writerow(('7', '8', '9'))  \n",
    "\n",
    "#### Reading Microsoft Excel Files\n",
    "pandas also supports reading tabular data stored in Excel 2003 (and higher) files using the `ExcelFile` class. Interally ExcelFile uses the `xlrd` and `openpyxl` packages, so you\n",
    "may have to install them first. To use ExcelFile, create an instance by passing a path to an xls or xlsx file:  \n",
    ">xls_file = pd.ExcelFile('data.xls')\n",
    "\n",
    "Data stored in a sheet can then be read into DataFrame using parse:  \n",
    ">table = xls_file.parse('Sheet1')  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TODO:\n",
    "\n",
    "* Interacting with HTML and Web APIs\n",
    "* Interacting with Databases\n",
    "* JSON\n",
    "* XML and HTML: Web Scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*******************************************\n",
    "Source:\n",
    "\n",
    "**Python for Data Analysis**  \n",
    "by Wes McKinney  \n",
    "Copyright © 2013 Wes McKinney. All rights reserved.  \n",
    "Printed in the United States of America.  \n",
    "Published by O’Reilly Media, Inc., 1005 Gravenstein Highway North, Sebastopol, CA 95472."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
